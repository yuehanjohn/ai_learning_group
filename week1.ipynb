{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Exercise 1: Implementing a Single Neuron**\n",
        "\n",
        "**Objective:** Understand the basic structure of a neuron.\n",
        "\n",
        "\n",
        "**Task:** Write a Python function that implements a single neuron. The neuron should take an input vector **X**\n",
        "and a weight vector **W**, apply a linear combination, and then apply a sigmoid activation function.\n",
        "\n",
        "$z = \\sigma\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)$\n",
        "\n",
        "Hint: Here is the sigmoid ($\\sigma$)function formula:\n",
        "\n",
        "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$"
      ],
      "metadata": {
        "id": "QleWkNk1Qr8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : array_like\n",
        "        Data point values for x-axis.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    z : float\n",
        "      a value between 0 and 1.\n",
        "\n",
        "    \"\"\"\n",
        "    ####### Your Code Here #########\n",
        "\n",
        "\n",
        "\n",
        "    ################################\n",
        "    return z\n",
        "\n",
        "def single_neuron(x, w, b):\n",
        "  \"\"\"\n",
        "  Implement the forward pass. Calculate the weighted sum (z = w*x + b)\n",
        "  Hint: Use np.dot for the weighted sum, then apply sigmoid\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : array_like\n",
        "      Data point values (input values).\n",
        "  w : array_like\n",
        "      Data point values (random weights).\n",
        "  b : float\n",
        "      Data point values (bias).\n",
        "  Returns\n",
        "  -------\n",
        "  z : float\n",
        "    a value between 0 and 1.\n",
        "  \"\"\"\n",
        "\n",
        "    ####### Your Code Here #########\n",
        "  z= np.dot(w,x)+b\n",
        "\n",
        "  ################################\n",
        "\n",
        "\n",
        "  return sigmoid(z)\n",
        "\n",
        "# Test the function with some inputs\n",
        "x = np.array([0.0, 0.5, -0.2, 0.1, 0.4, 0.3, -0.88, 0.4, 0.15, 0.69])\n",
        "w = np.array([0.4, 0.6, -0.8,0.2, 0.3, 0.4, 0.6, -0.5, -0.6, 0.9 ])\n",
        "b = 0.1\n",
        "output = single_neuron(x, w, b)\n",
        "print(\"Output:\", output)\n",
        "print(\"Expected Output:0.6509005438802886\")\n"
      ],
      "metadata": {
        "id": "cF5N6S3hRZRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2: Implementing a Simple Layer**\n",
        "\n",
        "**Objective:** Learn to stack neurons into a single layer.\n",
        "\n",
        "**Task:** Extend the single neuron into a layer of neurons. Implement a function that takes an input vector, a matrix of weights, and a vector of biases, and returns the output of the layer.\n"
      ],
      "metadata": {
        "id": "n9TslqjzS76b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer(x, W, b):\n",
        "\n",
        "    \"\"\"\n",
        "    Extend the single neuron into a layer of neurons.\n",
        "    Implement a function that takes an input vector, a matrix of weights, and a vector of biases, and returns the output of the layer.\n",
        "    ----------\n",
        "    x : array_like\n",
        "        Data point values (input values).\n",
        "    W :  array_like\n",
        "        multidimensional array of data point values (weights).\n",
        "    b : array_like\n",
        "        Data point values (bias).\n",
        "    Returns\n",
        "    -------\n",
        "    z : array_like\n",
        "       output values between 0 and 1.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ####### Your Code Here #########\n",
        "\n",
        "\n",
        "\n",
        "    ################################\n",
        "\n",
        "\n",
        "    return sigmoid(z)\n",
        "\n",
        "# Test the function with some inputs\n",
        "x = np.array([0.5, -0.2, 0.1])\n",
        "W = np.array([[0.4, 0.6, -0.8],\n",
        "              [0.2, -0.5, 0.3]])\n",
        "b = np.array([0.1, -0.2])\n",
        "output = layer(x, W, b)\n",
        "print(\"Layer Output:\", output)\n",
        "print(\"Expected Layer Output: [0.52497919 0.50749944]\")"
      ],
      "metadata": {
        "id": "rW9IPFzUTMMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3: Building a Two-Layer MLP**\n",
        "\n",
        "**Objective:** Understand how to combine multiple layers into a network.\n"
      ],
      "metadata": {
        "id": "my34jl-AVSKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def two_layer_mlp(x, W1, b1, W2, b2):\n",
        "  \"\"\"\n",
        "  Implement a simple two-layer MLP by stacking two layers from the previous exercise.\n",
        "  The first layer should have 3 neurons, and the second layer should have 2 neurons.\n",
        "  x : array_like\n",
        "      Data point values (input values).\n",
        "  W1 :  array_like\n",
        "      multidimensional array of data point values (weights).\n",
        "  b1 : array_like\n",
        "      Data point values (bias)\n",
        "\n",
        "  W2 :  array_like\n",
        "      multidimensional array of data point values (weights).\n",
        "  b2 : array_like\n",
        "      Data point values (bias)\n",
        "  Returns\n",
        "  -------\n",
        "  output : array_like\n",
        "      output values between 0 and 1.\n",
        "  \"\"\"\n",
        "  ####### Your Code Here #########\n",
        "  # First layer  Hint: use the layer function (neuron) you built previously.\n",
        "\n",
        "  # Second layer\n",
        "\n",
        "\n",
        "  ################################\n",
        "\n",
        "  return output\n",
        "\n",
        "# Test the function with some inputs\n",
        "x = np.array([0.5, -0.2, 0.1])\n",
        "W1 = np.array([[0.4, 0.6, -0.8],\n",
        "               [0.2, -0.5, 0.3],\n",
        "               [-0.3, 0.7, 0.9]])\n",
        "b1 = np.array([0.1, -0.2, 0.3])\n",
        "\n",
        "W2 = np.array([[0.3, -0.7, 0.5],\n",
        "               [-0.6, 0.2, 0.8]])\n",
        "b2 = np.array([0.2, -0.1])\n",
        "\n",
        "output = two_layer_mlp(x, W1, b1, W2, b2)\n",
        "print(\"MLP Output:\", output)\n",
        "print(\"Expected MLP Output: [0.56579959 0.5265988]\")\n"
      ],
      "metadata": {
        "id": "9h0D_cnmVOJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4: Example of MLP for MNIST Digit Recognition in TensorFlow**\n",
        "\n",
        "**Objective:** Python code for classifying the MNIST dataset using a Multilayer Perceptron (MLP) with Keras (TensorFlow backend). This MLP includes one hidden layer with ReLU activation and is trained using the Adam optimizer\n"
      ],
      "metadata": {
        "id": "UfthVtSCck5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape((x_train.shape[0], 28 * 28)) / 255.0\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(Dense(64, activation='relu' ))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "id": "eMyYoc9-kcFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "# Flatten the images from 28x28 to 784\n",
        "x_train = x_train.reshape(x_train.shape[0], 28 * 28).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], 28 * 28).astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the MLP model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(Dense(64, activation='relu'))  # Add more layers if needed\n",
        "model.add(Dense(10, activation='softmax'))  # Output layer for 10 classes (digits 0-9)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "# Optionally visualize predictions on a sample test image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choose a random image from test set\n",
        "random_index = np.random.randint(0, x_test.shape[0])\n",
        "plt.imshow(x_test[random_index].reshape(28, 28), cmap='gray')\n",
        "plt.title(f\"Predicted: {np.argmax(predictions[random_index])}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qeLK0HOlFxcR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}